{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a13e4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T00:48:54.753049Z",
     "start_time": "2022-07-17T00:48:54.742076Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a76189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T00:48:33.968256Z",
     "start_time": "2022-07-17T00:48:33.929944Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./dataset/trainprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a67a7187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T00:48:57.261709Z",
     "start_time": "2022-07-17T00:48:57.245996Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class windowDataset(Dataset):\n",
    "    def __init__(self, y, input_window=4, output_window=4, stride=4):\n",
    "        #총 데이터의 개수\n",
    "        L = y.shape[0]\n",
    "        #stride씩 움직일 때 생기는 총 sample의 개수\n",
    "        num_samples = (L - input_window - output_window) // stride + 1\n",
    "\n",
    "        #input과 output\n",
    "        X = np.zeros([input_window, num_samples])\n",
    "        Y = np.zeros([output_window, num_samples])\n",
    "\n",
    "        for i in np.arange(num_samples):\n",
    "            start_x = stride*i\n",
    "            end_x = start_x + input_window\n",
    "            X[:,i] = y[start_x:end_x]\n",
    "\n",
    "            start_y = stride*i + input_window\n",
    "            end_y = start_y + output_window\n",
    "            Y[:,i] = y[start_y:end_y]\n",
    "\n",
    "        X = X.reshape(X.shape[0], X.shape[1], 1).transpose((1,0,2))\n",
    "        Y = Y.reshape(Y.shape[0], Y.shape[1], 1).transpose((1,0,2))\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        \n",
    "        self.len = len(X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i, :-1], self.y[i,1:]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f5cc64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T00:48:58.043030Z",
     "start_time": "2022-07-17T00:48:58.000943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>Promotion1</th>\n",
       "      <th>Promotion2</th>\n",
       "      <th>Promotion3</th>\n",
       "      <th>Promotion4</th>\n",
       "      <th>Promotion5</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Unemployment_Class</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>ispeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1643690.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.106</td>\n",
       "      <td>True</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1641957.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1611968.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1409727.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1554806.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6250</th>\n",
       "      <td>45</td>\n",
       "      <td>2012-08-31</td>\n",
       "      <td>75.09</td>\n",
       "      <td>3.867</td>\n",
       "      <td>23641.30</td>\n",
       "      <td>6.00</td>\n",
       "      <td>92.93</td>\n",
       "      <td>6988.31</td>\n",
       "      <td>3992.13</td>\n",
       "      <td>8.684</td>\n",
       "      <td>False</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>734297.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>45</td>\n",
       "      <td>2012-09-07</td>\n",
       "      <td>75.70</td>\n",
       "      <td>3.911</td>\n",
       "      <td>11024.45</td>\n",
       "      <td>12.80</td>\n",
       "      <td>52.63</td>\n",
       "      <td>1854.77</td>\n",
       "      <td>2055.70</td>\n",
       "      <td>8.684</td>\n",
       "      <td>True</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>766512.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>45</td>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>67.87</td>\n",
       "      <td>3.948</td>\n",
       "      <td>11407.95</td>\n",
       "      <td>92.28</td>\n",
       "      <td>4.30</td>\n",
       "      <td>3421.72</td>\n",
       "      <td>5268.92</td>\n",
       "      <td>8.684</td>\n",
       "      <td>False</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>702238.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6253</th>\n",
       "      <td>45</td>\n",
       "      <td>2012-09-21</td>\n",
       "      <td>65.32</td>\n",
       "      <td>4.038</td>\n",
       "      <td>8452.20</td>\n",
       "      <td>92.28</td>\n",
       "      <td>63.24</td>\n",
       "      <td>2376.38</td>\n",
       "      <td>8670.40</td>\n",
       "      <td>8.684</td>\n",
       "      <td>False</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>723086.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254</th>\n",
       "      <td>45</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>64.88</td>\n",
       "      <td>3.997</td>\n",
       "      <td>4556.61</td>\n",
       "      <td>20.64</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1601.01</td>\n",
       "      <td>3288.25</td>\n",
       "      <td>8.684</td>\n",
       "      <td>False</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>713173.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6255 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Store        Date  Temperature  Fuel_Price  Promotion1  Promotion2  \\\n",
       "0         1  2010-02-05        42.31       2.572        0.00        0.00   \n",
       "1         1  2010-02-12        38.51       2.548        0.00        0.00   \n",
       "2         1  2010-02-19        39.93       2.514        0.00        0.00   \n",
       "3         1  2010-02-26        46.63       2.561        0.00        0.00   \n",
       "4         1  2010-03-05        46.50       2.625        0.00        0.00   \n",
       "...     ...         ...          ...         ...         ...         ...   \n",
       "6250     45  2012-08-31        75.09       3.867    23641.30        6.00   \n",
       "6251     45  2012-09-07        75.70       3.911    11024.45       12.80   \n",
       "6252     45  2012-09-14        67.87       3.948    11407.95       92.28   \n",
       "6253     45  2012-09-21        65.32       4.038     8452.20       92.28   \n",
       "6254     45  2012-09-28        64.88       3.997     4556.61       20.64   \n",
       "\n",
       "      Promotion3  Promotion4  Promotion5  Unemployment  IsHoliday  Year  \\\n",
       "0           0.00        0.00        0.00         8.106      False  2010   \n",
       "1           0.00        0.00        0.00         8.106       True  2010   \n",
       "2           0.00        0.00        0.00         8.106      False  2010   \n",
       "3           0.00        0.00        0.00         8.106      False  2010   \n",
       "4           0.00        0.00        0.00         8.106      False  2010   \n",
       "...          ...         ...         ...           ...        ...   ...   \n",
       "6250       92.93     6988.31     3992.13         8.684      False  2012   \n",
       "6251       52.63     1854.77     2055.70         8.684       True  2012   \n",
       "6252        4.30     3421.72     5268.92         8.684      False  2012   \n",
       "6253       63.24     2376.38     8670.40         8.684      False  2012   \n",
       "6254        1.50     1601.01     3288.25         8.684      False  2012   \n",
       "\n",
       "      Month  Unemployment_Class  Weekly_Sales  ispeak  \n",
       "0         2                   0    1643690.90       0  \n",
       "1         2                   0    1641957.44       0  \n",
       "2         2                   0    1611968.17       0  \n",
       "3         2                   0    1409727.59       0  \n",
       "4         3                   0    1554806.68       0  \n",
       "...     ...                 ...           ...     ...  \n",
       "6250      8                   3     734297.87       0  \n",
       "6251      9                   3     766512.66       0  \n",
       "6252      9                   3     702238.27       0  \n",
       "6253      9                   3     723086.20       0  \n",
       "6254      9                   3     713173.95       0  \n",
       "\n",
       "[6255 rows x 16 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27ea9cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T00:49:54.939855Z",
     "start_time": "2022-07-17T00:49:54.913524Z"
    }
   },
   "outputs": [],
   "source": [
    "window=windowDataset(train.query('Store==1')['Weekly_Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d31eb54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T00:58:48.016145Z",
     "start_time": "2022-07-17T00:58:47.995705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 4, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d101aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T01:19:59.221278Z",
     "start_time": "2022-07-17T01:19:59.184222Z"
    }
   },
   "outputs": [],
   "source": [
    "class TFModel(nn.Module):\n",
    "    def __init__(self,iw, ow, d_model, nhead, nlayers, dropout=0.5):\n",
    "        super(TFModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=nlayers) \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, d_model)\n",
    "        )\n",
    "        \n",
    "        self.linear =  nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, 1)\n",
    "        )\n",
    "\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(iw, (iw+ow)//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear((iw+ow)//2, ow)\n",
    "        ) \n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, srcmask):\n",
    "        src = self.encoder(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src.transpose(0,1), srcmask).transpose(0,1)\n",
    "        output = self.linear(output)[:,:,0]\n",
    "        output = self.linear2(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def gen_attention_mask(x):\n",
    "    mask = torch.eq(x, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "875e9097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T01:22:01.031872Z",
     "start_time": "2022-07-17T01:22:01.009060Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\n",
    "    'cpu')\n",
    "submissions=[]\n",
    "\n",
    "def training(data):\n",
    "    for i in tqdm(range(1, 46)):\n",
    "        model = TFModel(4,4,512, 8, 6, 0.4).to(device)\n",
    "        train_data = data.query(\n",
    "            'Store=={}'.format(i))['Weekly_Sales'].to_numpy().reshape(-1, 1)\n",
    "        MinMax = MinMaxScaler()\n",
    "        train_data = MinMax.fit_transform(train_data)\n",
    "        train_data = train_data.flatten()\n",
    "        window = windowDataset(train_data, input_window=4, output_window=4, stride=4)\n",
    "\n",
    "        lr = 1e-3\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        epoch = 100\n",
    "        progress = range(epoch)\n",
    "        for k in progress:\n",
    "            batchloss = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            train_x=torch.tensor(window.x)\n",
    "            train_y=torch.tensor(window.y)\n",
    "            src_mask = model.generate_square_subsequent_mask(train_x.shape[1]).to(device)\n",
    "    \n",
    "            result = model(train_x.float().to(device), src_mask)\n",
    "            loss = criterion(result, train_y[:, :, 0].float().to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            clear_output(wait=True)\n",
    "            test_x = torch.tensor(train_data[-4:]).reshape(1,-1,1).to(device).float().to(device)\n",
    "            src_mask_test = model.generate_square_subsequent_mask(\n",
    "                test_x.shape[1]).to(device)\n",
    "        prediction = model(test_x.float().to(device), src_mask_test)\n",
    "        prediction = prediction.detach().cpu().numpy()\n",
    "        pred = MinMax.inverse_transform(prediction)\n",
    "        pred = np.where(pred > 0, pred, 0).reshape(-1)\n",
    "        submissions=submissions.append(pred)\n",
    "    return submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abd4ad5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T01:22:11.372668Z",
     "start_time": "2022-07-17T01:22:01.332506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/45 [00:09<07:09,  9.77s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.16 GiB already allocated; 8.50 MiB free; 1.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(result, train_y[:, :, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 35\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m test_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_data[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py\\lib\\site-packages\\torch\\optim\\adam.py:125\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    123\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.16 GiB already allocated; 8.50 MiB free; 1.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "prediction=training(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bf0fc83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T01:29:35.092942Z",
     "start_time": "2022-07-17T01:29:35.057954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  Weekly_Sales\n",
       "0      1             0\n",
       "1      2             0\n",
       "2      3             0\n",
       "3      4             0\n",
       "4      5             0\n",
       "..   ...           ...\n",
       "175  176             0\n",
       "176  177             0\n",
       "177  178             0\n",
       "178  179             0\n",
       "179  180             0\n",
       "\n",
       "[180 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub=pd.read_csv('./dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d78fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['Weekly_Sales']=prediction\n",
    "sub.to_csv('./dataset/transformerpredict.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
